# RT-2_Vision-Language-Action Models Transfer Web Knowledge to Robotic Control

* 将动作标记成文本
* VLM使用动作令牌
* Resume mapping between Action Box and token.
* The model is very big, And it use more TPU on web.
* 指令套件

## 这篇论文的创新点是什么？

1. **视觉-语言-动作模型（VLA）的提出**: 作者提出了一种新颖的视觉-语言-动作模型（VLA），将视觉、语言和动作三个模态整合到一个统一的框架中。这是一种全新的方法，可以直接将在互联网规模数据上训练的视觉-语言模型整合到端到端的机器人控制中。
2. **动作作为文本标记的表示**: 通过将机器人的动作表示为文本标记，并与自然语言标记以相同的方式整合，作者提供了一种简单而通用的方法来实现机器人控制和大规模预训练的结合。**这种方法允许模型在自然语言理解和机器人动作控制之间进行无缝转换。**
3. **共同微调的策略**: 作者提出了在机器人轨迹数据和互联网规模视觉-语言任务上共同微调模型的方法。这种方法结合了机器人的实际控制需求和大规模的视觉-语言理解，增强了模型的泛化能力和灵活性。
4. **链式思维推理的引入**: RT-2 模型能够执行多阶段语义推理，例如找出哪个物体可以用作临时锤子，或者哪种饮料最适合疲倦的人。这种推理能力是一种新颖的特性，使模型能够在更复杂和多样化的场景中工作。
5. **广泛的评估和实验**: 通过进行6k评估试验，作者展示了 RT-2 在不同任务和环境中的性能。这种广泛的评估有助于证明模型的有效性和泛化能力。
6. **促进机器人控制的语义推理**: RT-2 模型的设计使其能够理解和执行更复杂的语义命令，如选择最小或最大的物体，或者执行多阶段任务。这种能力在机器人控制领域是相对新颖的，并可能推动该领域的进一步发展。

总的来说，这篇论文通过提出新颖的模型结构、表示方法、微调策略和推理能力，为将大规模预训练的视觉-语言理解转化为实际的机器人控制提供了新的路径。这些创新点共同推动了机器人控制和多模态学习领域的发展。


## 如何泛化模型的使用场景？

在 "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control" 这篇论文中，泛化模型的使用场景主要通过以下几个方面实现：

1. **大规模预训练**: 通过在互联网规模的视觉-语言数据上预训练模型，RT-2 能够从大量的现实世界场景中学习通用的知识和理解。这有助于模型在未见过的环境和任务中表现良好。
2. **共同微调**: 作者提出了在机器人轨迹数据和互联网规模视觉-语言任务上共同微调模型的方法。这种方法结合了机器人的实际控制需求和大规模的视觉-语言理解，使模型能够在不同的机器人控制场景中泛化。
3. **动作作为文本标记**: 通过将动作表示为文本标记，并将其与自然语言标记以相同的方式整合，模型能够将自然语言理解和机器人动作控制结合在一起。这使得模型能够理解和执行机器人训练数据中不存在的命令，从而增强了泛化能力。
4. **链式思维推理**: RT-2 还能够执行多阶段语义推理，例如找出哪个物体可以用作临时锤子，或者哪种饮料最适合疲倦的人。这种推理能力使模型能够在更复杂和多样化的场景中工作。
5. **评估和实验**: 通过广泛的评估（6k评估试验），作者展示了 RT-2 在不同的任务和环境中的性能。这包括对新物体的泛化、解释未在训练数据中出现的命令、以及对用户命令进行初步推理等。

总的来说，通过结合大规模预训练、共同微调、将动作与语言整合、以及链式思维推理等方法，RT-2 模型实现了对不同机器人控制场景的泛化。这使得模型不仅能够在特定任务上表现良好，还能够适应和理解许多不同的环境和指令，从而在更广泛的使用场景中表现出色。
